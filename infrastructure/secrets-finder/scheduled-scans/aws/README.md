# Scheduled scans of git-based repositories

<br /><br />

## Table of Contents
1. [Introduction](#introduction)
2. [Infrastructure](#infrastructure)
3. [Scanning configuration](#scanning-configuration)
4. [Generated reports](#generated-reports)
5. [Deployment instructions](#deployment-instructions)

<br/><br/>

## Introduction
This folder holds the infrastructure for the scheduled secrets scanning solution. The system is designed for use with git-based repositories belonging to an organization and deployed on different source code management platforms. The open-source tool [TruffleHog](https://github.com/trufflesecurity/trufflehog) carries out the scanning of repositories for secrets. The tool is executed within dedicated instances that are set up on a predefined interval.

> **NOTE:**\
> As part of the first and current release of secrets-finder, the solution relies on AWS. As such, the documentation below is tailored to this platform. The maintainers of the project aim to provide support for other cloud providers in the next releases.

<br/><br/>

## Infrastructure
The infrastructure consists of several key components that are deployed in AWS. The main components are described below.

### CloudWatch Event
This resource is used to trigger the CodeBuild Build Project responsible of scheduling the scans at a predefined interval. The interval is set to every Monday by default, but can be adjusted as needed.

### CodeBuild Build Project
This resource has its own build specification, formatted based on a predefined template, and populated based on the scans to be performed. As part of its operations, the Build Project schedules the start of an EC2 instance for the first scan, drops the instance from the Terraform state, and proceeding similarly for the next scan, and so on. If something goes wrong, the Build Project offers the possibility to send notifications to a preconfigured SNS topic.

### S3 Bucket
A bucket is used to store both the configuration belonging to each scan defined, and the results of the scans performed, as well as the logs generated by the instances. This bucket is expected to be created using the [`storage` module](/infrastructure/setup/aws/storage).

### Secrets Manager
This service is notably used to store the credentials used to fetch the repositories to scan. Each reference loaded by the `automation` module points to secrets stored before hands in Secrets Maager using the [`secrets` module](/infrastructure/setup/aws/secrets).

### EC2 Instances
These instances are used to perform the scans. They are started by the CodeBuild Build Project and are terminated once the scan is completed, unless the user specifies not to do so. The instances are configured with the necessary permissions to access the repositories to scan, and to push the results to the S3 bucket. Each scan can hold its own configuration (see below).

<br/><br/>

## Scanning configuration
The `${var.scans}` variable in the `automation` module specifies the list of scans to perform. Each scan is defined by a set of options described below:
- `identifier`: this represents the unique identifier of the scan, and should ideally represents a human-readable name to easily identify which jobs and findings belong to the same context
- `scm`: the source code management platform where the repositories to be scanned are stored (supported values are `github`, `azure_devops`, and `custom` (for all other source code management platforms))
- `credentials_reference`: the name of the secret stored in Secrets Manager and holding the credentials to use when fetching the repositories to scan (should be an object with `username` and `password` keys, both of type string)
- `ec2_instance_type`: the type of EC2 instance to use when performing the scan (this option is typically used to scale the infrastructure for each scan based on the number of repositories to cover)
- `files`: an array containing local paths to all the files that users want to inject to the EC2 instance when performing the scan (this allows to specify pre- and post-scan scripts to execute)
- `repositories_to_scan`: the path to the file containing the list of repositories to scan (this file should conform to the `/configuration/secrets-finder/scanner/scan-configuration.schema.json` schema; if not present, it is assumed that a pre-scan script will generate the list of repositories to scan and save such list on the `/home/secrets-finder/scanner/repositories_to_scan.json` file on the EC2 instance)
- `terminate_instance_on_error`: if set to `true`, the instance is terminated if an error occurs during the scan â€“ including during execution of pre- and post-scan scripts (default is `true`)
- `terminate_instance_after_scan`: if set to `true`, the instance is terminated after the scan is completed (default is `true`)
- `report_only_verified`: if set to `true`, only verified secrets are reported in the results (default is `false`)

> **NOTE:**\
> The endpoint specified in the repositories_to_scan.json file should be a template string denoting the endpoint to call when cloning repositories. The template string should contains those two variables: `organization` and `repository`. For example, a valid endpoint for GitHub would be: `https://github.com/{organization}/{repository}`.

<br/><br/>

## Generated reports
Each time a scan is performed, a report is generated and persisted. This report contains a JSON object made of the following elements:
- `scan_type`: always `detection` with ongoing scans
- `start`: date (in ISO format) indicating when the scan started
- `end`: date (in ISO format) indicating when the scan finished
- `status`: either `succces` if the scan could be performed, or `failure` otherwise
- `scan_mode`: `verified` if only verified secrets are reported, `all` otherwise (the number of findings reported does not influence this value)
- `scan_context`: always `repository` for scheduled scans
- `scan_uuid`: unique identifier representing the scan performed
- `scan_identifier`: matches the identifier specified in the configuration of the scan
- `scm`: matches the scm specified in the configuration of the scan
- `results`: an array containing as many entries as we have repositories scanned in an iteration

The `results` key holds an array where each object reports the following information:
- `scan_uuid`: the identifier representing the scan of the repository (different than `scan_uuid` field at top-level)
- `start`: same as the top-level key of the same name
- `end`: same as the top-level key of the same name
- `organization`: the name of the GitHub organization the repository belongs to
- `repository`: the name of the repository scanned
- `status`: either `success` if the scan could be performed, or `failure` otherwise (the number of findings reported does not influence this value)
- `metadata`: any metadata provided in the `repositories_to_scan.json` file for the repository scanned
- `findings`: an array of findings as returned by TruffleHog, if any found

<br/><br/>

## Deployment instructions
To set up the infrastructure, please proceed with the following steps.

### Registration of required secrets in AWS Secrets Manager
Using the [`secrets` module](/infrastructure/secrets-finder/setup/aws/secrets), you must store all the secrets needed to perform a scan: namely, the credentials used to reach the different source code management plaforms to scan, and the organizations hosted there.

### Registration of Datadog API token in AWS Secrets Manager
You have the possibility to use Datadog for reporting on EC2 instance activity. For this, you need to store the API key in Secrets Manager, e.g., by using the [`secrets` module](/infrastructure/secrets-finder/setup/aws/secrets) provided. Then, you should specify the `${var.datadog_api_key_reference}` variable, which represents the name of the secret stored in Secrets Manager and holding the API key to use. You should also specify the other variables related to Datadog in the `terraform.tfvars` file.

### Deployment of the AWS infrastructure
> **Note:**\
> It is assumed that you have already [installed Terraform](https://developer.hashicorp.com/terraform/downloads) and configured your AWS credentials accordingly for the profile you want to use.

Then, navigate to the [`automation`](/infrastructure/secrets-finder/scheduled-scans/aws/automation) directory.

To configure the S3 backend for Terraform, modify the `s3.tfbackend` file by setting the appropriate values. Be sure to reference the correct `<aws_profile>` AWS profile in the `profile` key.

Then, initialize Terraform:
```bash
terraform init -backend-config=s3.tfbackend
```

> **IMPORTANT:**\
> To successfully deploy the infrastructure, it is assumed that the S3 Bucket holding the remote states already exists. This also holds for the DynamoDB Table listing the locks. You are responsible for the creation of such resources. We recommend reusing the same bucket and table across all modules of secrets-finder. In such case, make sure to specify a different path for each module.

Next, create a `terraform.tfvars` file and set the required variables. This file as well as the [README.md](/infrastructure/secrets-finder/ongoing-scans/aws/README.md) file provided alongside the module provide valuable information about the purpose of each variable.

Lastly, review the changes to be made and, if satisfactory, proceed with deploying the infrastructure by following the steps below:
```bash
# Review changes
terraform plan

# Deploy changes
terrafrom apply -auto-approve
```

Upon successful completion, the following outputs should be available:
- `codebuild_arn`
- `event_rule_arn`
